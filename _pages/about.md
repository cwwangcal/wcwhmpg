---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Welcome to my personal homepage! I am Chuwei Wang, a Ph.D. student from the 
Department of Computing and Mathematical Sciences ([CMS](https://www.cms.caltech.edu/)), Caltech. I am fortunate 
to be advised by Professor [Anima Anandkumar](http://tensorlab.cms.caltech.edu/users/anima/) and 
[Andrew Stuart](http://stuart.caltech.edu/). Prior to joining Caltech in 2023, 
I obtained my B.S. degree in computational mathematics from the School of Mathematical Sciences ([SMS](http://english.math.pku.edu.cn/)), 
Peking University, where I was advised by Professor [Liwei Wang](http://www.liweiwang-pku.com/) and 
[Di He](https://dihe-pku.github.io/).

My research goal is to develop practical and powerful machine learning tools for scientific
problems through theory, algorithm design, and high performance computing. I am also 
fascinated with infinite-dimensional objects.

<p style="margin-bottom: 0;">Specifically, I am currently interested in:</p>
<ul style="line-height: 0.6;">  <!-- modify 1.2 to change linespacing-->
  <li>Operator learning.</li>
  <li>Application of deep learning in physical sciences.</li>
  <li>Closure modeling (a.k.a. coarse-grained modeling, reduce order modeling, etc).</li>
  <li>Sampling and optimal transport.</li>
</ul>

<!-- Specifically, I am curently interested in 
1. Operator learning.
1. Application of deep learning in physical sciences.
1. Closure modeling (a.k.a. coarse-grained modeling, reduce order modeling, etc).
1. Sampling and optimal transport. -->
<!-- specifice ai4 physical sciences, e.g. quantum chemistry...  -->

I love working with collaborators from various scientific backgrounds. 
If you are interested in my research, please feel free to contact me!




News
======
- **Aug. 2024** Give a talk on data-driven closure models at [IAIFI summer workshop](https://iaifi.org/summer-workshop).
<!-- - **Jul. 2024** Giving a talk at DDCR MURI Meeting. -->
- **Aug. 2024** Our [new work](https://arxiv.org/abs/2408.05177) on closure modeling is released on arxiv. 
We reveal the fundamental shortcomings of existing learning-based closure models and propose a new learning framework,
achieving both tremendous speedup and higher accuracy.
- **May 2024** We extend Forward Laplacian to accomodate general high-order differential operators.
The [paper](https://arxiv.org/abs/2402.09730) is accepted by <em style="color: #8B4513;">ICLR 2024 workshop on AI4DifferentialEquations</em>.
- **Feb. 2024** Our [Forward Laplacian paper](https://www.nature.com/articles/s42256-024-00794-x) is accepted by <em style="color: #8B4513;">Nature Machine Intelligence</em>.
We achieves more than 20x speedup on ground-state energy computing (QMC) for qunatum systems with $$\sim10^2$$ electrons.


Selected Recent Works
======

<div style="display: flex; align-items: center; margin-bottom: 2px;">
  <div style="flex: 0 0 250px; padding-right: 20px;">
    <img src="/images/papers/closure.png" alt="Publication Image" style="width: 500px; height: auto;">
  </div>
  <div style="flex: 1;">
    <h3><a href="https://arxiv.org/abs/2408.05177" target="_blank">Beyond Closure Models: Learning Chaotic-Systems via Physics-Informed Neural Operators</a></h3>
    <span style="font-size: 85%;"><strong>Chuwei Wang</strong>, Julius Berner, Zongyi Li, Di Zhou, Jiayun Wang, Jane Bae, Anima Anandkumar</span><br>
    <span style="font-size: 85%;"><em>Preprint</em></span><br>
    <!-- <span style="font-size: 80%;">We introduce a novel neural network parameterization that induces low-rank simplicity bias and enhances semantic feature learning in generative representation learning frameworks...</span> -->
    <!-- <span style="font-size: 95%;">
      <a href="https://arxiv.org/abs/2408.05177" target="_blank">[Slides]</a> 
      <a href="https://arxiv.org/abs/2408.05177" target="_blank">[Poster]</a> 
      <a href="/images/papers/closure.png" target="_blank">[Code]</a>
    </span> -->
  </div>
</div>


<div style="display: flex; align-items: center; margin-bottom: 2px;">
  <div style="flex: 0 0 250px; padding-right: 20px;">
    <img src="/images/papers/FL_mechanism.png" alt="Publication Image" style="width: 500px; height: auto;">
  </div>
  <div style="flex: 1;">
    <h3><a href="https://arxiv.org/abs/2402.09730" target="_blank">
    DOF: Accelerating High-order Differential Operators with Forward Propagation</a></h3>
    <span style="font-size: 85%;">Ruichen Li*, <strong>Chuwei Wang*</strong>, Haotian Ye*, Di He, Liwei Wang</span><br>
    <span style="font-size: 85%;"><em>ICLR 2024 workshop on AI4DifferentialEquations</em></span><br>
  </div>
</div>

<div style="display: flex; align-items: center; margin-bottom: 2px;">
  <div style="flex: 0 0 250px; padding-right: 20px;">
    <img src="/images/papers/FL_CH2.png" alt="Publication Image" style="width: 500px; height: auto;">
  </div>
  <div style="flex: 1;">
    <h3><a href="https://arxiv.org/abs/2307.08214" target="_blank">
    A computational framework for neural network-based variational Monte Carlo with Forward Laplacian</a></h3>
    <span style="font-size: 85%;">Ruichen Li*, Haotian Ye*, Du Jiang, Xuelan Wen, <strong>Chuwei Wang</strong>, Zhe Li, Xiang Li, Di He, Ji Chen, Weiluo Ren, Liwei Wang</span><br>
    <span style="font-size: 85%;"><em>Nature Machine Intelligence</em></span><br>
  </div>
</div>

<div style="display: flex; align-items: center; margin-bottom: 2px;">
  <div style="flex: 0 0 250px; padding-right: 20px;">
    <img src="/images/papers/linf_pinn.png" alt="Publication Image" style="width: 500px; height: auto;">
  </div>
  <div style="flex: 1;">
    <h3><a href="https://arxiv.org/abs/2206.02016" target="_blank">
    Is L^2 Physics-Informed Loss Always Suitable for Training Physics-Informed Neural Network?</a></h3>
    <span style="font-size: 80%;"><strong>Chuwei Wang*</strong>, Shanda Li*, Di He, Liwei Wang</span><br>
    <span style="font-size: 80%;"><em>NeurIPS 2022</em></span><br>
  </div>
</div>

<!-- <div style="display: flex; align-items: center; margin-bottom: 2px;">
  <div style="flex: 0 0 250px; padding-right: 20px;">
    <img src="/images/papers/500x300.png" alt="Publication Image" style="width: 500px; height: auto;">
  </div>
  <div style="flex: 1;">
    <h3><a href="https://arxiv.org/abs/2408.05177" target="_blank">Residual connections harm generative representation learning</a></h3>
    <strong>Authors:</strong> Chuwei Wang, Julius Berner, Zongyi Li, Di Zhou, Jiayun Wang, Jane Bae, Anima Anandkumar<br>
    <em>In review</em><br>
    <span style="font-size: 80%;">We introduce a novel neural network parameterization that induces low-rank simplicity bias and enhances semantic feature learning in generative representation learning frameworks...</span>
  </div>
</div> -->



<!-- Site-wide configuration
------
The m -->



Services
======
- Reviewer of JMLR, Nature Machine Intelligence (NMI), Neurips, ICLR.
- Organizing Committee member of [Keller Colloquium](https://www.cms.caltech.edu/news-events/keller-colloquium).


Selected Awards
======
- **Jun. 2023** Peking University Excellent Graduate.
- **Aug. 2022** Bronze Medal (ranked No. 6 in total), S.T. Yau College Student Mathematics Competition, analysis and partial differential equations individual.
- **May 2021** Elite undergraduate training program of Applied Mathematics and Statistics.
- **Dec. 2020** First Prize, Cheinese National College Student Physics Competition
- **Dec. 2020** First Prize (ranked No. 14 in total), Cheinese National College Student Mathematics Competition

